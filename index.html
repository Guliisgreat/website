<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Li Gu</title>

  <meta name="author" content="Li Gu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Li Gu</name>
                  </p>
                  <p>I am a research engineer at Noah‚Äôs Ark Lab, Huawei Canada in Toronto, supervised by <a href="https://users.encs.concordia.ca/~wayang/">Prof. Wang Yang</a>. 
                    I obtained M. Eng in Computer Engineering from University of Toronto and B. Eng in Electrical & Computer Engineering from Shanghai Jiao Tong University. 
                  <p>
                  <p>
                    My research interests are at the intersection of computer vision and machine learning. 
                    Recently, I've been focusing on how to enable machine learning models to adapt to novel environments rapidly and efficiently, including areas in meta-learning, few-shot learning, continual learning, and prompt learning.  
                  </p>
                  <!-- <p>
                    I obtained Master's degree in Computer Engineering from University of Toronto and B. Eng in Electrical & Computer Engineering from Shanghai Jiao Tong University. 
                  </p> -->
                  <p style="text-align:center">
                    <a href="mailto:guliisworking@hotmail.com">Email</a> &nbsp/&nbsp
                    <!--                 <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                    <!--                 <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.ca/citations?user=crdHC0sAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <!--                 <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                    <a href="https://github.com/Guliisgreat">Github</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/li-gu/">Linkedin</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/profile.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publication</heading>
                  <!-- <p>
                    My primary research interests lie at the intersection of machine learning and computer vision.
                  </p> -->
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/meta_dmoe.png" alt="meta_dmoe" width="350" height="150" style="border-style: none">
                </td>
                <td width="75%" valign="middle">

                  <papertitle>Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts
                  </papertitle>

                  <br>
                  Tao Zhong*, Zhixiang Chi*, <strong>Li Gu*</strong>, Yang Wang, Yuanhao Yu, Jin Tang
                  <br>
                  <em>NeurIPS</em>, 2022.
                  <br>

                  <a href="https://arxiv.org/abs/2210.03885">Paper</a> /
                  <a href="data/Meta-DMoE_NeurIPS2022.pptx">Slides</a> /
                  <a href="https://github.com/n3il666/Meta-DMoE">Code</a>
                  <p></p>

                  <p>
                    Proposed a new framework for unsupervised test-time adaption toward domain shift; Formulated the
                    adaptation process as knowledge distillation and meta-learned the scheme of knowledge aggregation from
                    multiple source domains.
                  </p>

                </td>
              </tr>


              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/orbit_2022.png" alt="orbit_2022" width="350" height="150" style="border-style: none">
                </td>
                <td width="75%" valign="middle">

                  <papertitle>Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022
                  </papertitle>

                  <br>
                  <strong>Li Gu</strong>, Zhixiang Chi*, Huan Liu*, Yuanhao Yu, Yang Wang
                  <br>
                  <em>CVPR</em> 2022 VisWiz workshop.
                  <br>                  

                  <a href="https://arxiv.org/abs/2210.00174">Paper</a> /
                  <a href="data/orbit_2022.pdf">Slides</a> /
                  <a href="https://github.com/Guliisgreat/ORBIT-2022-winner-method">Code</a>
                  <p></p>

                  <p>Extended ProtoNet-based few-shot image classification approaches into the video domain.
                    <br><br>
                    <a style="color:red">The winning team</a> at the <a href="https://eval.ai/web/challenges/challenge-page/1438/overview">ORBIT few-shot object recognition challenge</a>; Awarded cash prizes of 2,500 USD.
                  </p>

                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/meta_fscil.png" alt="meta_fscil" width="350" height="200" style="border-style: none">
                </td>
                <td width="75%" valign="middle">

                  <papertitle>MetaFSCIL: A Meta-Learning Approach for Few-Shot Class Incremental Learning
                  </papertitle>

                  <br>
                  Zhixiang Chi, <strong>Li Gu</strong>, Huan Liu, Yang Wang, Yuanhao Yu, Jin Tang
                  <br>
                  <em>CVPR</em>, 2022.
                  <br>

                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chi_MetaFSCIL_A_Meta-Learning_Approach_for_Few-Shot_Class_Incremental_Learning_CVPR_2022_paper.pdf">Paper</a> /
                  <a href="data/MetaFSCIL_cvpr2022_talk.pdf">Talk</a> 
                  <p></p>

                  <p>
                    Proposed  a bi-level optimization-based meta-learning approach to learning how to learn incrementally in the Few-Shot Class Incremental Learning(FSCIL) setting.
                  </p>

                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/data-free-replay.png" alt="free-replay" width="350" height="150" style="border-style: none">
                </td>
                <td width="75%" valign="middle">

                  <papertitle>Few-Shot Class-Incremental Learning via Entropy-Regularized Data-Free Replay
                  </papertitle>

                  <br>
                  Huan Liu, <strong>Li Gu</strong>, Zhixiang Chi, Yang Wang, Yuanhao Yu, Jun Chen, Jin Tang
                  <br>
                  <em>ECCV</em>, 2022.
                  <br>

                  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136840144.pdf">Paper</a> /
                  <!-- <a href="/data/MetaFSCIL_cvpr2022_talk.pdf">Talk</a>  -->
                  <p></p>

                  <p>
                    Proposed a replay-based Few-Shot Class Incremental Learning(FSCIL) framework that can synthesize data by a generator without storing real data during each continual learning session. 
                    Imposed entropy regularization in the generator training to encourage more uncertain examples to be utilized in knowledge distillation. 
                  </p>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/dmm-net.png" alt="dmm-net" width="350" height="200" style="border-style: none">
                </td>
                <td width="75%" valign="middle">

                  <papertitle>DMM-Net: Differentiable Mask-Matching Network for Video Object Segmentation
                  </papertitle>

                  <br>
                  Xiaohui Zeng*, Renjie Liao*, <strong>Li Gu</strong>, Yuwen Xiong, Sanja Fidler, Raquel Urtasun
                  <br>
                  <em>ICCV</em>, 2019.
                  <br>

                  <a href="https://arxiv.org/abs/1909.12471">Paper</a> /
                  <a href="https://github.com/ZENGXH/DMM_Net">Code</a> 
                  <p></p>

                  <p>
                    Proposed the differentiable mask-matching network (DMM-Net) for solving the video object segmentation problem where the initial object masks are provided.
                  </p>

                </td>
              </tr>

              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/APD-BNN.png" alt="APD-BNN" width="350" height="250" style="border-style: none">
                </td>
                <td width="75%" valign="middle">

                  <papertitle>Adversarial Distillation of Bayesian Neural Network Posteriors
                  </papertitle>

                  <br>
                  Xiaohui Zeng*, Renjie Liao*, <strong>Li Gu</strong>, Yuwen Xiong, Sanja Fidler, Raquel Urtasun
                  <br>
                  <em>ICML</em>, 2018.
                  <br>

                  <a href="https://arxiv.org/abs/1806.10317?context=cs">Paper</a> /
                  <a href="https://github.com/wangkua1/apd_public">Code</a> /
                  <a href="https://www.paulvicol.com/pdfs/AdversarialPosteriorDistillation_Slides.pdf">Slides</a> /
                  <a href="https://www.paulvicol.com/pdfs/ICML_2018_APD_Poster.pdf">Poster</a> 

                  <p></p>

                  <p>
                    Proposed an efficient framework for using a small Generative Adversarial
                    Network (GAN) to store MCMC samples of the posterior from a large Bayesian Neural Network.
                    
                  </p>

                </td>
              </tr>





</body>

</html>